{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogoTc21cDSa0"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRW3DbRdDfd_"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FACoi7GPDczq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkCsw88eDwMT"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMqkEarVD1tQ"
      },
      "outputs": [],
      "source": [
        "def generate_embedding_with_ada(text: str) -> list:\n",
        "    embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "    embedding = embeddings_model.embed_query(text)\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5Daux-CD4RV"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def qualitative_evaluation_with_model(model, answer: str, question: str, reference_answer: str, aspect: str) -> str:\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"answer\", \"question\", \"reference_answer\"],\n",
        "        template=(\n",
        "            f\"Avalie a {aspect} da resposta '{{answer}}' para a pergunta '{{question}}', \"\n",
        "            f\"em comparação com a resposta de referência '{{reference_answer}}'.\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    llm_chain = LLMChain(llm=model, prompt=prompt_template)\n",
        "\n",
        "    evaluation = llm_chain.run(answer=answer, question=question, reference_answer=reference_answer)\n",
        "    return evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2--PcTQDA0Qq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from scipy.spatial.distance import cosine\n",
        "from typing import List, Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML7sxBTNA5Ca"
      },
      "outputs": [],
      "source": [
        "class DatasetLoader:\n",
        "    def __init__(self, data_path: str):\n",
        "        self.data_path = data_path\n",
        "\n",
        "    def load_data(self):\n",
        "        data = pd.read_csv(self.data_path)\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgaH1XxgA8Bq"
      },
      "outputs": [],
      "source": [
        "class DataPreprocessor:\n",
        "    def preprocess(self, data):\n",
        "        data['question'] = data['question'].str.lower().str.strip()\n",
        "        data['answer'] = data['answer'].str.lower().str.strip()\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLEW97u_A95S"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ModelRunner:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def get_answer(self, image, question): #resposta pode vir direto do dataset\n",
        "        response = self.model.predict(image, question)\n",
        "        return response\n",
        "\n",
        "    def evaluate_answer(self, answer: str, question: str, reference_answer: str) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Avalia a resposta em múltiplas métricas qualitativas, usando diferentes prompts para cada uma.\n",
        "        \"\"\"\n",
        "        relevance_prompt = (\n",
        "            f\"Avalie a relevância da resposta '{answer}' para a pergunta '{question}', \"\n",
        "            f\"considerando a resposta de referência '{reference_answer}'.\"\n",
        "        )\n",
        "\n",
        "        coherence_prompt = (\n",
        "            f\"Avalie a coerência da resposta '{answer}' em relação à pergunta '{question}', \"\n",
        "            f\"com base na resposta de referência '{reference_answer}'.\"\n",
        "        )\n",
        "\n",
        "        clarity_prompt = (\n",
        "            f\"Avalie a clareza da resposta '{answer}' dada para a pergunta '{question}', \"\n",
        "            f\"comparando com a resposta de referência '{reference_answer}'.\"\n",
        "        )\n",
        "\n",
        "        relevance_evaluation = self.model(relevance_prompt)\n",
        "        coherence_evaluation = self.model(coherence_prompt)\n",
        "        clarity_evaluation = self.model(clarity_prompt)\n",
        "\n",
        "        return {\n",
        "            \"relevance\": relevance_evaluation,\n",
        "            \"coherence\": coherence_evaluation,\n",
        "            \"clarity\": clarity_evaluation\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv9gNA62BPdL"
      },
      "outputs": [],
      "source": [
        "class MetricsEvaluator:\n",
        "    def __init__(self, reference_answers: List[str], model_answers: List[str], questions: List[str]):\n",
        "        self.reference_answers = reference_answers\n",
        "        self.model_answers = model_answers\n",
        "        self.questions = questions\n",
        "\n",
        "    def accuracy(self):\n",
        "        return accuracy_score(self.reference_answers, self.model_answers)\n",
        "\n",
        "    def f1_score(self):\n",
        "        return f1_score(self.reference_answers, self.model_answers, average='weighted')\n",
        "\n",
        "    def semantic_relevance(self):\n",
        "        similarities = []\n",
        "        for ref, model_ans in zip(self.reference_answers, self.model_answers):\n",
        "            ref_embedding = generate_embedding_with_ada(ref)\n",
        "            model_embedding = generate_embedding_with_ada(model_ans)\n",
        "            similarity = 1 - cosine(ref_embedding, model_embedding)\n",
        "            similarities.append(similarity)\n",
        "        return np.mean(similarities)\n",
        "\n",
        "    def qualitative_evaluation(self):\n",
        "        evaluations = []\n",
        "        for question, ref, model_ans in zip(self.questions, self.reference_answers, self.model_answers):\n",
        "            eval_text = qualitative_evaluation_with_gpt4o(model_ans, question, ref)\n",
        "            evaluations.append(eval_text)\n",
        "        return evaluations\n",
        "\n",
        "    def evaluate(self):\n",
        "        metrics = {\n",
        "            \"accuracy\": self.accuracy(),\n",
        "            \"f1_score\": self.f1_score(),\n",
        "            \"semantic_relevance\": self.semantic_relevance(),\n",
        "            \"qualitative_evaluation\": self.qualitative_evaluation()\n",
        "        }\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSUEfHWzBRSP"
      },
      "outputs": [],
      "source": [
        "class ResultsReporter:\n",
        "    def __init__(self, metrics: Dict[str, float]):\n",
        "        self.metrics = metrics\n",
        "\n",
        "    def report(self):\n",
        "        print(\"Relatório de Resultados:\")\n",
        "        for metric, value in self.metrics.items():\n",
        "            if metric != \"qualitative_evaluation\":\n",
        "                print(f\"{metric}: {value:.2f}\")\n",
        "            else:\n",
        "                print(\"\\nAvaliações Qualitativas das Respostas:\")\n",
        "                for eval_text in value:\n",
        "                    print(f\"- {eval_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3suPE8js-Wso"
      },
      "outputs": [],
      "source": [
        "def main(data_path, model):\n",
        "    loader = DatasetLoader(data_path)\n",
        "    data = loader.load_data()\n",
        "\n",
        "    preprocessor = DataPreprocessor()\n",
        "    data = preprocessor.preprocess(data)\n",
        "\n",
        "    model_runner = ModelRunner(model)\n",
        "    model_answers = [\n",
        "        model_runner.get_answer(row['image'], row['question'])\n",
        "        for _, row in data.iterrows()\n",
        "    ]\n",
        "\n",
        "    evaluator = MetricsEvaluator(data['answer'].tolist(), model_answers, data['question'].tolist())\n",
        "    metrics = evaluator.evaluate()\n",
        "\n",
        "    reporter = ResultsReporter(metrics)\n",
        "    reporter.report()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQXWNy3VBLDF"
      },
      "outputs": [],
      "source": [
        "main(\"dataset.csv\", \"gpt-4o-mini\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}